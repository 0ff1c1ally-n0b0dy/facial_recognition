{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to get closer to what the Facebook paper says: https://arxiv.org/pdf/2012.12877.pdf\n",
    "#DeiT has good depth and width and it is better than resnet apparently, which only has good depth.\n",
    "#DeiT also has attention in it btw.\n",
    "\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.init import *\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pdb import set_trace\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from skimage import io, transform\n",
    "from tqdm import trange, tqdm\n",
    "import csv \n",
    "import glob \n",
    "import dlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "\n",
    "#Imports inside facenet scripts that need to be loaded here on google colab.\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "import torch\n",
    "import hashlib\n",
    "import shutil\n",
    "import tempfile\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on device: {device}')\n",
    "\n",
    "#####################\n",
    "#Global valriables\n",
    "####################\n",
    "#The following variable determines how many pictures are fed at once into the NN (this is made for speed). \n",
    "#Facebook does 4096.\n",
    "batches=32   \n",
    "#The following path points to the folder that contains the images with faces only.\n",
    "data_dir=r\"C:\\Users\\mihnea.andrei\\Python scripts\\resnet_andrei\\images_faces\"\n",
    "\n",
    "def imshow(inp,title=None):\n",
    "  ############\n",
    "  #Imshow for tensor\n",
    "  #############\n",
    "    inp=inp.numpy().transpose([1,2,0])\n",
    "    mean=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "    inp=std*inp+mean\n",
    "    inp=np.clip(inp,0,1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.0001)\n",
    "\n",
    "#The following function times the NN.\n",
    "def timming(since):\n",
    "    time_elapsed=time.time()-since\n",
    "    minutes=time_elapsed // 60\n",
    "    seconds=time_elapsed%60\n",
    "    return \"%d m:%d s\"%(minutes,seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you use the MTCNN code that does the random transformations also, you do not need all those transformations again \n",
    "#(maybe) except the horizonta flip - the angle from which the picture is taken.\n",
    "data_transforms={\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  \n",
    "        #transforms.ColorJitter(brightness=0.5,contrast=0.5),\n",
    "        #transforms.GaussianBlur(kernel_size=(5, 9), sigma=(3, 7)),\n",
    "        #transforms.RandomAdjustSharpness(sharpness_factor=15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ]),\n",
    "    'eval': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "}\n",
    "phases=list(data_transforms.keys())\n",
    "image_datasets={x:datasets.ImageFolder(os.path.join(data_dir,x),data_transforms[x]) for x in list(data_transforms.keys())}\n",
    "dataloaders={x:torch.utils.data.DataLoader(image_datasets[x],batch_size=batches,shuffle=True) for x in list(data_transforms.keys())}\n",
    "data_sizes={x:len(image_datasets[x]) for x in list(data_transforms.keys())}\n",
    "class_names=image_datasets['train'].classes \n",
    "\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "out=utils.make_grid(inputs)\n",
    "imshow(out,title=\",\".join([class_names[i] for i in classes.tolist()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eeb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        q = q * self.scale           #Google does not do this\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention,Mlp_block=Mlp\n",
    "                 ,init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention_block(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        #Google has a layerScale here\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        #Google has a layer scale here followed by a drop path.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x \n",
    "    \n",
    "class Layer_scale_init_Block(nn.Module):\n",
    "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "    # with slight modifications\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention,Mlp_block=Mlp\n",
    "                 ,init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention_block(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "#Google has a more general implementation of the following class (paralxn in general) and it has a different order in the\n",
    "#layers.\n",
    "    \n",
    "class Layer_scale_init_Block_paralx2(nn.Module):\n",
    "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "    # with slight modifications\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention,Mlp_block=Mlp\n",
    "                 ,init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.norm11 = norm_layer(dim)\n",
    "        self.attn = Attention_block(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.attn1 = Attention_block(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.norm21 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.mlp1 = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        self.gamma_1_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        self.gamma_2_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.gamma_1*self.attn(self.norm1(x))) + self.drop_path(self.gamma_1_1 * self.attn1(self.norm11(x)))\n",
    "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x))) + self.drop_path(self.gamma_2_1 * self.mlp1(self.norm21(x)))\n",
    "        return x\n",
    "        \n",
    "class Block_paralx2(nn.Module):\n",
    "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "    # with slight modifications\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention,Mlp_block=Mlp\n",
    "                 ,init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.norm11 = norm_layer(dim)\n",
    "        self.attn = Attention_block(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.attn1 = Attention_block(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.norm21 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.mlp1 = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x))) + self.drop_path(self.attn1(self.norm11(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x))) + self.drop_path(self.mlp1(self.norm21(x)))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class hMLP_stem(nn.Module):\n",
    "    \"\"\" hMLP_stem: https://arxiv.org/pdf/2203.09795.pdf\n",
    "    taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "    with slight modifications\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224,  patch_size=16, in_chans=3, embed_dim=768,norm_layer=nn.SyncBatchNorm):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = torch.nn.Sequential(*[nn.Conv2d(in_chans, embed_dim//4, kernel_size=4, stride=4),\n",
    "                                          norm_layer(embed_dim//4),\n",
    "                                          nn.GELU(),\n",
    "                                          nn.Conv2d(embed_dim//4, embed_dim//4, kernel_size=2, stride=2),\n",
    "                                          norm_layer(embed_dim//4),\n",
    "                                          nn.GELU(),\n",
    "                                          nn.Conv2d(embed_dim//4, embed_dim, kernel_size=2, stride=2),\n",
    "                                          norm_layer(embed_dim),\n",
    "                                         ])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "    \n",
    "class vit_models(nn.Module):\n",
    "    \"\"\" Vision Transformer with LayerScale (https://arxiv.org/abs/2103.17239) support\n",
    "    taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "    with slight modifications\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224,  patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, global_pool=None,\n",
    "                 block_layers = Block,\n",
    "                 Patch_layer=PatchEmbed,act_layer=nn.GELU,\n",
    "                 Attention_block = Attention, Mlp_block=Mlp,\n",
    "                dpr_constant=True,init_scale=1e-4,\n",
    "                mlp_ratio_clstk = 4.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout_rate = drop_rate\n",
    "\n",
    "            \n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = Patch_layer(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "\n",
    "        dpr = [drop_path_rate for i in range(depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            block_layers(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=0.0, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                act_layer=act_layer,Attention_block=Attention_block,Mlp_block=Mlp_block,init_values=init_scale)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "\n",
    "        \n",
    "            \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "    \n",
    "    def get_num_layers(self):\n",
    "        return len(self.blocks)\n",
    "    \n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "            \n",
    "        for i , blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.forward_features(x)\n",
    "        \n",
    "        if self.dropout_rate:\n",
    "            x = F.dropout(x, p=float(self.dropout_rate), training=self.training)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e8070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below number is 160 because the faces extracted by MTCNN are 160x160.\n",
    "img_size=160\n",
    "#The number of names that the model will see (population of romania over 14).\n",
    "num_classes=data_sizes[\"train\"]\n",
    "#Variables inside the network. Please check examples that start at line 271 here: \n",
    "#https://github.com/facebookresearch/deit/blob/main/models_v2.py\n",
    "embed_dim=192\n",
    "depth=12\n",
    "num_heads=3\n",
    "mlp_ratio=4\n",
    "\n",
    "model_fit=vit_models(num_classes=num_classes,img_size=img_size,patch_size=batches,\n",
    "                     embed_dim=embed_dim,depth=depth,num_heads=num_heads,mlp_ratio=mlp_ratio,qkv_bias=True,\n",
    "                    norm_layer=partial(nn.LayerNorm,eps=10**(-6)),\n",
    "                    block_layers=Layer_scale_init_Block).to(device)\n",
    "\n",
    "num_epochs=25\n",
    "lr=3*10**(-3)\n",
    "weight_decay=0.01\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "#Facebook does AdamW. Please see paper.\n",
    "#optimizer=optim.SGD(model_fit.parameters(),lr=10**(-2),momentum=0.9)\n",
    "optimizer=torch.optim.AdamW(model_fit.parameters(),lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=weight_decay)\n",
    "#lr_decay=lr_scheduler.StepLR(optimizer,step_size=7,gamma=0.1)\n",
    "lr_decay=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c276dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_weights=copy.deepcopy(model_fit.state_dict())\n",
    "best_acc=0\n",
    "since=time.time()\n",
    "printing_iter=10**(1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-\"*100)\n",
    "    print(\"Training for epoch %d/%d started\"%(epoch+1,num_epochs))\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    train_losses=[]\n",
    "    train_accs=[]\n",
    "    counter=0\n",
    "\n",
    "    for x, label in dataloaders[\"train\"]:\n",
    "        x=x.to(device)\n",
    "        label=label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output=model_fit(x)\n",
    "        loss=criterion(output,label)\n",
    "        \n",
    "        output=torch.exp(output)\n",
    "        _, pred=output.topk(1,dim=1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_decay.step()\n",
    "        \n",
    "        last_loss=loss.item()\n",
    "        train_losses.append(last_loss)    \n",
    "        last_acc=torch.sum(pred==label.data).double()\n",
    "        train_accs.append(last_acc)\n",
    "\n",
    "        counter+=1\n",
    "        if counter%printing_iter==0:\n",
    "            print(\"Train completion:%.2f%%, time:%s, avg loss: %.2f, avg acc:%.2f%%, last loss: %.2f\"\n",
    "                  %(100*counter*batches/data_sizes[\"train\"],timming(since),np.mean(train_losses),\n",
    "                    100*np.mean(train_accs),last_loss))\n",
    "    \n",
    "    training_avg_loss=np.mean(train_losses)\n",
    "    training_avg_accuracy=100*np.mean(train_accs)\n",
    "    print(\"-\"*100)\n",
    "    print(\"Eval for epoch %d/%d started\"%(epoch+1,num_epochs))\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    since=time.time()\n",
    "    counter=0\n",
    "    eval_losses=[]\n",
    "    eval_accs=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, label in dataloaders[\"eval\"]:\n",
    "            \n",
    "            output=model_fit(x)\n",
    "            loss=criterion(output,label)\n",
    "            \n",
    "            output=torch.exp(output)\n",
    "            _, pred=output.topk(1,dim=1)\n",
    "            \n",
    "            \n",
    "            last_loss=loss.item()\n",
    "            eval_losses.append(last_loss)\n",
    "            last_acc=torch.sum(pred==label.data).double()\n",
    "            eval_accs.append(last_acc)\n",
    "            \n",
    "            counter+=1\n",
    "            if counter%printing_iter==0:\n",
    "                print(\"Eval completion:%.2f%%, time:%s, avg loss: %.2f, avg acc:%.2f%%, last loss: %.2f\"\n",
    "                      %(100*counter*batches/data_sizes[\"eval\"],timming(since),np.mean(eval_losses),\n",
    "                        100*np.mean(eval_accs),last_loss))\n",
    "            \n",
    "    eval_avg_loss=np.mean(eval_losses)\n",
    "    eval_avg_acc=100*np.mean(eval_accs)\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    print(\"Completed epoch %d/%d in %s, avg train loss: %.2f, avg train acc: %.2f%%, avg eval loss: %.2f, avg eval acc: %.2f\"%\n",
    "          (epoch+1,num_epochs,timming(since),training_avg_loss,training_avg_accuracy,eval_avg_loss,eval_avg_acc))\n",
    "    print(\"-\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path=r\"C:\\Users\\mihnea.andrei\\Python scripts\\DeiT_Facebook\"\n",
    "save_model_file=r\"\\DeiT,embed_dim=%d,depth=%d,num_heads=%d,mlp_ratio=%d,train_loss=%.4f,train_accs=%.2f%%.pt%\"%\n",
    "    (embed_dim,depth,num_heads,mlp_ratio,np.mean(train_losses),100*np.mean(train_accs))\n",
    "\n",
    "torch.save(model_fit,save_model_path+save_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training losses\")\n",
    "plt.plot(train_losses,label=\"train losses\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_accs=100*np.cumsum(train_accs)/list(range(len(train_accs)))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Cumulative training average\")\n",
    "plt.plot(train_accs,label=\"train accuracies\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094dd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
